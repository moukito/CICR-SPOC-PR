# ========================================
# AI Model Configuration
# ========================================

# Use legacy implementation (0 = modern, 1 = legacy)
USE_LEGACY=0

# Default LLM Model Key
# Options: mistral, llama3, deepseek
# Leave empty to use config default (llama3)
DEFAULT_LLM_KEY=llama3

# Default Embedding Model Key
# Options: minilm, mpnet, jina
# Leave empty to use config default (minilm)
DEFAULT_EMBEDDING_KEY=bge

# ========================================
# Vector Database Backend Configuration
# ========================================

# Vector Backend Selection
# Options:
#   - llamaindex: In-memory indexing (default)
#   - chroma: Persistent vector DB with separation of concerns
#   - agno: AGNO framework with LlamaIndex embeddings
VECTOR_BACKEND=llamaindex

# ========================================
# Chroma Vector Database Settings
# (Used when VECTOR_BACKEND=chroma)
# ========================================

# Directory where Chroma persists the vector database
CHROMA_DIR=chroma_storage

# Name of the Chroma collection to use
CHROMA_COLLECTION=documents

# Embedding model key to use with Chroma
# Options: minilm, mpnet, jina
# Leave empty to use DEFAULT_EMBEDDING_KEY
EMBEDDING_MODEL_KEY=minilm

# Skip document loading in runtime when using Chroma backend
# Set to 1 to skip loading (assumes DB is pre-populated via ingest_chroma.py)
# Set to 0 to force document loading even in Chroma mode
CHROMA_SKIP_LOAD=1

